Index: wip.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy\nimport numpy as np\nimport pandas as pd\n\nfrom main import simulate_data\nfrom rf.RandomFeaturesGenerator import RandomFeaturesGenerator\nfrom helpers.random_features import RandomFeatures\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom helpers.marcenko_pastur import MarcenkoPastur\n\n\n# mohammad_is_wrong = RandomFeatures.naive_linear_single_underlying()\n\ndef smart_eigenvalue_decomposition(features: np.ndarray,\n                                   T: int = None):\n    \"\"\"\n\n    :param features: features used to create covariance matrix T x P\n    :param T: Weight used to normalize matrix\n    :return: Left eigenvectors PxT and eigenvalues without zeros\n    \"\"\"\n    [T_true, P] = features.shape\n    T = T_true if T is None else T\n\n    if P > T:\n        print('complex regime')\n        covariance = features @ features.T / T\n\n    else:\n        print('regular regime')\n        covariance = features.T @ features / T\n\n    eigval, eigvec = np.linalg.eigh(covariance)\n    eigvec = eigvec[:, eigval > 10 ** (-10)]\n    eigval = eigval[eigval > 10 ** (-10)]\n\n    if P > T:\n        eigvec = np.matmul(features.T, eigvec * ((eigval * T) ** (-1 / 2)).reshape(1, -1))\n\n    return eigval, eigvec\n\n\ndef smart_w_matrix(features: np.ndarray,\n                   eigenvalues: np.ndarray,\n                   eigenvectors: np.ndarray,\n                   shrinkage_list: np.ndarray):\n    \"\"\"\n    (z+Psi)^{-1} = U (z+lambda)^{-1}U' + z^{-1} (I - UU')\n    we compute S'(z+Psi)^{-1} S= S' (\n    :param features:\n    :param eigenvalues:\n    :param eigenvectors:\n    :param shrinkage_list:\n    :return:\n    \"\"\"\n    [T,P] = features.shape\n    projected_features = eigenvectors.T @ features.T\n    stuff_divided = [(1 / (eigenvalues.reshape(-1, 1) + z)) * projected_features for z in shrinkage_list]\n\n    W = [projected_features.T @ x_ for x_ in stuff_divided]\n\n    cov_left_over = features @ features.T - projected_features.T @ projected_features\n\n    W_list = [(W[i] + (1 / shrinkage_list[i]) * cov_left_over)/T for i in range(len(shrinkage_list))]\n    return W_list\n\n\ndef leave_two_out_estimator_vectorized(labels: np.ndarray,\n                                      features: np.ndarray,\n                                      shrinkage_list: np.ndarray) -> float:\n    \"\"\"\n    # Implement leave two out estimators\n    # For any matrix A independent of t_1 and t_2\n    # E[R_{t_2+1} S_{t_1}'A S_{t_2} R_{t_1+1}]\\ =\n    \\  \\beta'\\Psi A_left (\\hat \\Psi + zI)^{-1} A_right \\Psi \\beta\\\n\n\n    :param labels: Variables we wish to predict\n    :param features: Signals we use to predict variables\n    :param shrinkage_list:\n    :return: Unbiased estimator\n    \"\"\"\n    eigenvalues, eigenvectors = smart_eigenvalue_decomposition(features)\n\n    W = smart_w_matrix(features=features,\n                       eigenvalues=eigenvalues,\n                       eigenvectors=eigenvectors,\n                       shrinkage_list=shrinkage_list)\n\n    [T, P] = np.shape(features)\n\n    estimator = 0\n    num = (T - 1) / 2 # divded by T to account for W normalization\n\n    labels_squared = (labels.reshape(-1,1)*np.squeeze(labels)).reshape(1,-1)\n\n    W_diag = [(1 - np.diag(w)).reshape(-1, 1) * (1 - np.diag(w)) for w in W]\n    [np.fill_diagonal(w, 0) for w in W] # get rid of diagonal elements not in the calculations\n    estimator_list = [np.sum(labels_squared * W[i].reshape(1, -1) / (W_diag[i].reshape(1, -1) - W[i].reshape(1, -1) ** 2))/num \\\n                      for i in range(len(shrinkage_list))]\n\n    return estimator_list\n\n\n# def leave_two_out_estimator_resolvent(labels: np.ndarray,\n#                                       features: np.ndarray,\n#                                       z: float,\n#                                       resolvent: bool = True,\n#                                       smart_inv: bool = False) -> float:\n#     \"\"\"\n#     # Implement leave two out estimators\n#     # For any matrix A independent of t_1 and t_2\n#     # E[R_{t_2+1} S_{t_1}'A S_{t_2} R_{t_1+1}]\\ =\n#     \\  \\beta'\\Psi A_left (\\hat \\Psi + zI)^{-1} A_right \\Psi \\beta\\\n#\n#     :param labels: Variables we wish to predict\n#     :param features: Signals we use to predict variables\n#     :param z: Ridge shrinkage\n#     :param resolvent: Estimate quadratic form with resolvent\n#     :param smart_inv: Use smart covariance estimation using sprectral decompositoin\n#     :return: Unbiased estimator\n#     \"\"\"\n#     [T, P] = np.shape(features)\n#\n#     if resolvent == T:\n#         if smart_inv == True:\n#             inv = smart_cov_inv(features, z, T)\n#         if smart_inv == False:\n#             inv = np.linalg.pinv(features.T @ features / T + z * np.eye(P))\n#\n#     estimator = 0\n#     num = (T - 1) * T / 2\n#\n#     for i in range(T):\n#         for j in range(i + 1, T):\n#             w_ij = features[i, :].reshape(1, -1) @ inv @ features[j, :].reshape(-1, 1) / T\n#             w_ii = features[i, :].reshape(1, -1) @ inv @ features[i, :].reshape(-1, 1) / T\n#             w_jj = features[j, :].reshape(1, -1) @ inv @ features[j, :].reshape(-1, 1) / T\n#             w_estimator = w_ij / ((1 - w_ii) * (1 - w_jj) - w_ij ** 2)\n#             estimator += T * w_estimator * labels[i] * labels[j]\n#\n#     estimator = estimator / num\n#     return estimator\n\n#\n# def leave_two_out_estimator_fast(labels: np.ndarray,\n#                                  features: np.ndarray,\n#                                  A: np.ndarray) -> float:\n#     \"\"\"\n#     # Implement leave two out estimators\n#     # For any matrix A independent of t_1 and t_2\n#     # E[R_{t_2+1} S_{t_1}'A S_{t_2} R_{t_1+1}]\\ =\\  \\beta'\\Psi A \\Psi \\beta\\\n#     :param labels: Variables we wish to predict\n#     :param features: Signals we use to predict variables\n#     :param A: Weighting matrix to help with estimation\n#     :return: Unbiased estimator\n#     \"\"\"\n#     [sample_size, number_features] = np.shape(features)\n#     estimator = 0\n#     num = (sample_size - 1) * sample_size / 2\n#\n#     if np.prod(A == np.eye(number_features)):\n#         for i in tqdm(range(sample_size - 1)):\n#             estimator += labels[i + 1:, :].T @ (features[i + 1:, :] @ features[i, :].reshape(-1, 1)) * labels[i]\n#     else:\n#         for i in tqdm(range(sample_size - 1)):\n#             estimator += labels[i + 1:, :].T @ (features[i + 1:, :] @ A @ features[i, :].reshape(-1, 1)) * labels[i]\n#\n#     estimator = estimator / num\n#     return estimator\n\n\n# def leave_one_out(labels: np.ndarray,\n#                   features: np.ndarray,\n#                   z: float,\n#                   smart_inv: bool = False) -> float:\n#     [T, P] = np.shape(features)\n#\n#     estimator = 0\n#     num = (T - 1)  / 2\n#\n#     projected_features = eigenvectors.T @ features.T/T\n#     stuff_divided = [(1 / (eigenvalues.reshape(-1, 1) + z)) * projected_features for z in shrinkage_list]\n#\n#     W = [projected_features.T @ x_ for x_ in stuff_divided]\n#\n#     cov_left_over = features @ features.T - projected_features.T @ projected_features\n#\n#     W_list = [(W[i] + (1 / shrinkage_list[i]) * cov_left_over)/T for i in range(len(shrinkage_list))]\n#\n#     for i in range(T):\n#         features_t = np.delete(features, i, 0)\n#         if smart_inv:\n#             inv = smart_inv(features_t, z, T)\n#         else:\n#             inv = np.linalg.pinv(features_t.T @ features_t / T + z * np.eye(P))\n#\n#         estimator += labels[i] * (\n#                 features[i].reshape(1, -1) @ inv @ features[i + 1:, :].T @ labels[i + 1:].reshape(-1, 1))\n#\n#     estimator = estimator / num\n#     return estimator\n\n\n#\n# def smart_cov_inv(features: np.ndarray, z: float, T: int) -> np.ndarray:\n#     \"\"\"\n#\n#     :param features: signals\n#     :param z: ridge shrinkage\n#     :param T: number of sample\n#     :return: computationally cheap covariance matrix\n#     \"\"\"\n#\n#     [T_true, P] = features.shape\n#     covariance = features.T @ features / T\n#\n#     if T > P:\n#         print('regular regime')\n#         inv = eigvec @ np.diag((z + eigval) ** (-1)) @ eigvec.T\n#\n#     else:\n#         inv = eigvec @ np.diag((z + eigval) ** (-1)) @ eigvec.T + (np.eye(P) - eigvec @ eigvec.T) * (1 / z)\n#\n#     return inv\n\n\nif __name__ == '__main__':\n\n\n    seed = 0\n    sample_size = 100\n    number_features_ = 10000\n    beta_and_psi_link_ = 2\n    noise_size_ = 0\n    activation_ = 'linear'\n    number_neurons_ = 1\n    shrinkage_list = np.linspace(0.1,10,100).tolist()\n\n    labels, features, beta_dict, psi_eigenvalues = simulate_data(seed=seed,\n                                                                 sample_size=sample_size,\n                                                                 number_features_=number_features_,\n                                                                 beta_and_psi_link_=beta_and_psi_link_,\n                                                                 noise_size_=noise_size_,\n                                                                 activation_=activation_,\n                                                                 number_neurons_=number_neurons_)\n\n    # estimators = leave_two_out_estimator_vectorized(labels=labels,\n    # features=features,\n    # shrinkage_list=shrinkage_list)\n    [T,P] = np.shape(features)\n    c_ = P / T\n    shrinkage_list = np.array(shrinkage_list)\n    mp = MarcenkoPastur.marcenko_pastur(c_,shrinkage_list)\n    eigenvalues, eigenvectors = smart_eigenvalue_decomposition(features, T)\n    eigenvalues = eigenvalues  # add np.zeros(P - len(eigenvalues))\n\n    empirical_stieltjes = (1 / (eigenvalues.reshape(-1, 1) + shrinkage_list.reshape(1, -1))).sum(0) / P \\\n                          + (P - len(eigenvalues)) / shrinkage_list / P\n\n    plt.plot(shrinkage_list,empirical_stieltjes)\n    plt.plot(shrinkage_list,mp)\n    plt.legend(['Empirical','True'])\n    plt.xlabel('Shrinkage z')\n    plt.ylabel('CDF value')\n    plt.title(f'complexity={c_}')\n    plt.show()\n\n    # run_experiment(seed=0,\n    #                sample_size=100,\n    #                number_features_=10000,\n    #                beta_and_psi_link_=2,\n    #                noise_size_=0,\n    #                activation_='linear',\n    #                number_neurons_=1,\n    #                shrinkage_list=np.exp(np.arange(-10, 10, 5)).tolist())\n\n    # T = np.linspace(100, 1000, 10)\n    # P = np.linspace(100, 1000, 10)\n    # error_beta2_psi2 = []\n    # error_beta2_psi = []\n    # for t in T:\n    #     seed = 0\n    #     sample_size = int(t)\n    #     number_features_ = 100\n    #     beta_and_psi_link_ = 2\n    #     noise_size_ = 0\n    #     activation_ = 'linear'\n    #     number_neurons_ = 1\n    #     shrinkage_list = np.exp(np.arange(-10, 10, 5)).tolist()\n    #\n    #     labels, features, beta_dict, psi_eigenvalues = simulate_data(seed=seed,\n    #                                                                  sample_size=sample_size,\n    #                                                                  number_features_=number_features_,\n    #                                                                  beta_and_psi_link_=beta_and_psi_link_,\n    #                                                                  noise_size_=noise_size_,\n    #                                                                  activation_=activation_,\n    #                                                                  number_neurons_=number_neurons_)\n    #     A = np.eye(number_features_)\n    #\n    #     estimator = leave_two_out_estimator_fast(labels, features, A)\n    #     true_value = psi_eigenvalues ** 2 @ beta_dict[0] ** 2\n    #     percentage_error = np.abs(estimator - true_value) / true_value\n    #\n    #     estimator_21 = np.mean(labels ** 2)\n    #     true_value_21 = psi_eigenvalues @ beta_dict[0] ** 2\n    #     percentage_error_21 = np.abs(estimator_21 - true_value_21) / true_value_21\n    #     error_beta2_psi.append(percentage_error_21[0])\n    #\n    #     error_beta2_psi2.append(percentage_error[0])\n    #\n    # plt.plot(T, error_beta2_psi2)\n    # plt.title(f'Error for fixed P = {number_features_} increasing T')\n    # plt.ylabel('Error in percentage')\n    # plt.xlabel('Value of T')\n    # plt.show()\n    #\n    # plt.plot(T, error_beta2_psi)\n    # plt.title(f'Error for fixed P = {number_features_} increasing T')\n    # plt.ylabel('Error in percentage')\n    # plt.xlabel('Value of T')\n    # plt.show()\n    #\n    # error_beta2_psi2 = []\n    # error_beta2_psi = []\n    #\n    # for p in P:\n    #     seed = 0\n    #     sample_size = 100\n    #     number_features_ = int(p)\n    #     beta_and_psi_link_ = 2\n    #     noise_size_ = 0\n    #     activation_ = 'linear'\n    #     number_neurons_ = 1\n    #     shrinkage_list = np.exp(np.arange(-10, 10, 5)).tolist()\n    #\n    #     labels, features, beta_dict, psi_eigenvalues = simulate_data(seed=seed,\n    #                                                                  sample_size=sample_size,\n    #                                                                  number_features_=number_features_,\n    #                                                                  beta_and_psi_link_=beta_and_psi_link_,\n    #                                                                  noise_size_=noise_size_,\n    #                                                                  activation_=activation_,\n    #                                                                  number_neurons_=number_neurons_)\n    #     A = np.eye(number_features_)\n    #\n    #     estimator = leave_two_out_estimator_fast(labels, features, A)\n    #     true_value = psi_eigenvalues ** 2 @ beta_dict[0] ** 2\n    #     percentage_error = np.abs(estimator - true_value) / true_value\n    #\n    #     estimator_21 = np.mean(labels ** 2)\n    #     true_value_21 = psi_eigenvalues @ beta_dict[0] ** 2\n    #     percentage_error_21 = np.abs(estimator_21 - true_value_21) / true_value_21\n    #     error_beta2_psi.append(percentage_error_21[0])\n    #\n    #     error_beta2_psi2.append(percentage_error[0])\n    #\n    # plt.plot(P, error_beta2_psi2)\n    # plt.title(f'Error for fixed T = {sample_size} increasing P')\n    # plt.ylabel('Error in percentage')\n    # plt.xlabel('Value of P')\n    # plt.show()\n    #\n    # plt.plot(P, error_beta2_psi)\n    # plt.title(f'Error for fixed T = {sample_size} increasing P')\n    # plt.ylabel('Error in percentage')\n    # plt.xlabel('Value of P')\n    # plt.show()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/wip.py b/wip.py
--- a/wip.py	(revision 600494533a1deff64e7c18c7e70407c10e786344)
+++ b/wip.py	(date 1668497429322)
@@ -54,7 +54,7 @@
     :param shrinkage_list:
     :return:
     """
-    [T,P] = features.shape
+    [T, P] = features.shape
     projected_features = eigenvectors.T @ features.T
     stuff_divided = [(1 / (eigenvalues.reshape(-1, 1) + z)) * projected_features for z in shrinkage_list]
 
@@ -62,13 +62,13 @@
 
     cov_left_over = features @ features.T - projected_features.T @ projected_features
 
-    W_list = [(W[i] + (1 / shrinkage_list[i]) * cov_left_over)/T for i in range(len(shrinkage_list))]
+    W_list = [(W[i] + (1 / shrinkage_list[i]) * cov_left_over) / T for i in range(len(shrinkage_list))]
     return W_list
 
 
 def leave_two_out_estimator_vectorized(labels: np.ndarray,
-                                      features: np.ndarray,
-                                      shrinkage_list: np.ndarray) -> float:
+                                       features: np.ndarray,
+                                       shrinkage_list: np.ndarray) -> float:
     """
     # Implement leave two out estimators
     # For any matrix A independent of t_1 and t_2
@@ -91,14 +91,15 @@
     [T, P] = np.shape(features)
 
     estimator = 0
-    num = (T - 1) / 2 # divded by T to account for W normalization
+    num = (T - 1) / 2  # divided by T to account for W normalization
 
-    labels_squared = (labels.reshape(-1,1)*np.squeeze(labels)).reshape(1,-1)
+    labels_squared = (labels.reshape(-1, 1) * np.squeeze(labels)).reshape(1, -1)
 
     W_diag = [(1 - np.diag(w)).reshape(-1, 1) * (1 - np.diag(w)) for w in W]
-    [np.fill_diagonal(w, 0) for w in W] # get rid of diagonal elements not in the calculations
-    estimator_list = [np.sum(labels_squared * W[i].reshape(1, -1) / (W_diag[i].reshape(1, -1) - W[i].reshape(1, -1) ** 2))/num \
-                      for i in range(len(shrinkage_list))]
+    [np.fill_diagonal(w, 0) for w in W]  # get rid of diagonal elements not in the calculations
+    estimator_list = [
+        np.sum(labels_squared * W[i].reshape(1, -1) / (W_diag[i].reshape(1, -1) - W[i].reshape(1, -1) ** 2)) / num \
+        for i in range(len(shrinkage_list))]
 
     return estimator_list
 
@@ -227,8 +228,6 @@
 
 
 if __name__ == '__main__':
-
-
     seed = 0
     sample_size = 100
     number_features_ = 10000
@@ -236,7 +235,7 @@
     noise_size_ = 0
     activation_ = 'linear'
     number_neurons_ = 1
-    shrinkage_list = np.linspace(0.1,10,100).tolist()
+    shrinkage_list = np.linspace(0.1, 10, 100).tolist()
 
     labels, features, beta_dict, psi_eigenvalues = simulate_data(seed=seed,
                                                                  sample_size=sample_size,
@@ -249,19 +248,19 @@
     # estimators = leave_two_out_estimator_vectorized(labels=labels,
     # features=features,
     # shrinkage_list=shrinkage_list)
-    [T,P] = np.shape(features)
+    [T, P] = np.shape(features)
     c_ = P / T
     shrinkage_list = np.array(shrinkage_list)
-    mp = MarcenkoPastur.marcenko_pastur(c_,shrinkage_list)
+    mp = MarcenkoPastur.marcenko_pastur(c_, shrinkage_list)
     eigenvalues, eigenvectors = smart_eigenvalue_decomposition(features, T)
     eigenvalues = eigenvalues  # add np.zeros(P - len(eigenvalues))
 
     empirical_stieltjes = (1 / (eigenvalues.reshape(-1, 1) + shrinkage_list.reshape(1, -1))).sum(0) / P \
                           + (P - len(eigenvalues)) / shrinkage_list / P
 
-    plt.plot(shrinkage_list,empirical_stieltjes)
-    plt.plot(shrinkage_list,mp)
-    plt.legend(['Empirical','True'])
+    plt.plot(shrinkage_list, empirical_stieltjes)
+    plt.plot(shrinkage_list, mp)
+    plt.legend(['Empirical', 'True'])
     plt.xlabel('Shrinkage z')
     plt.ylabel('CDF value')
     plt.title(f'complexity={c_}')
